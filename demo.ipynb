{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde770d8-91d2-4270-9e2e-e5be7cd5418b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "from argparse import Namespace\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import utils_model\n",
    "import utils_gradio\n",
    "import utils_attn\n",
    "\n",
    "\n",
    "# Update rcParams to disable axes\n",
    "mpl.rcParams['axes.spines.left'] = False\n",
    "mpl.rcParams['axes.spines.right'] = False\n",
    "mpl.rcParams['axes.spines.top'] = False\n",
    "mpl.rcParams['axes.spines.bottom'] = False\n",
    "mpl.rcParams['xtick.bottom'] = False\n",
    "mpl.rcParams['xtick.top'] = False\n",
    "mpl.rcParams['xtick.labelbottom'] = False\n",
    "mpl.rcParams['ytick.left'] = False\n",
    "mpl.rcParams['ytick.right'] = False\n",
    "mpl.rcParams['ytick.labelleft'] = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b25c1b7-1b4d-4b9e-be68-02b5a9723534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for microsoft/Phi-3.5-vision-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3.5-vision-instruct.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shaoyent/mambaforge/envs/gradio_lvlm/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:590: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "The repository for microsoft/Phi-3.5-vision-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3.5-vision-instruct.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n",
      "The repository for microsoft/Phi-3.5-vision-instruct contains custom code which must be executed to correctly load the model. You can inspect the repository content at https://hf.co/microsoft/Phi-3.5-vision-instruct.\n",
      "You can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n",
      "\n",
      "Do you wish to run the custom code? [y/N]  y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:02<00:00,  1.37s/steps]\n"
     ]
    }
   ],
   "source": [
    "model_name_or_path = \"microsoft/Phi-3.5-vision-instruct\" \n",
    "# model_name_or_path = \"llava-hf/llava-1.5-7b-hf\"\n",
    "\n",
    "args = Namespace(\n",
    "    model_name_or_path=model_name_or_path,\n",
    "    load_4bit=False,\n",
    "    load_8bit=False,\n",
    ")\n",
    "processor, model = utils_model.get_processor_model(args)\n",
    "\n",
    "utils_gradio.processor = processor\n",
    "utils_gradio.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c90dfa2b-b1c9-4864-a690-1c89c0ef979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"<|image_{i}|>\\nWhat's unusual?\"\n",
    "# image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "# raw_image = Image.open(requests.get(image_file, stream=True).raw)\n",
    "raw_image = Image.open('extreme_ironing.jpg')\n",
    "\n",
    "image_process_mode = \"Default\"\n",
    "state, _, _, _ = utils_gradio.add_text(None, prompt, raw_image, image_process_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32135964-ca7a-4d24-807d-b27d457fdb0d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "total images must be the same as the number of image tags, got 0 image tags and 1 images",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m top_p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.7\u001b[39m\n\u001b[1;32m      3\u001b[0m max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m----> 5\u001b[0m state, _ \u001b[38;5;241m=\u001b[39m \u001b[43mutils_gradio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlvlm_bot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m recovered_image \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mrecovered_image\n\u001b[1;32m      8\u001b[0m fig \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39mfigure()\n",
      "File \u001b[0;32m~/src/lvlm-interpret/intellabs-lvlm-interpret/utils_gradio.py:125\u001b[0m, in \u001b[0;36mlvlm_bot\u001b[0;34m(state, temperature, top_p, max_new_tokens)\u001b[0m\n\u001b[1;32m    122\u001b[0m prompt_len \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mprompt_len\n\u001b[1;32m    123\u001b[0m image \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mimage\n\u001b[0;32m--> 125\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    126\u001b[0m input_ids \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39minput_ids\n\u001b[1;32m    127\u001b[0m img_idx \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mwhere(input_ids\u001b[38;5;241m==\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mimage_token_index)[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-vision-instruct/4a0d683eba9f1d0cbfb6151705d1ee73c25a80ca/processing_phi3_v.py:377\u001b[0m, in \u001b[0;36mPhi3VProcessor.__call__\u001b[0;34m(self, text, images, padding, truncation, max_length, return_tensors)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    376\u001b[0m     image_inputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 377\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_images_texts_to_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inputs\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-vision-instruct/4a0d683eba9f1d0cbfb6151705d1ee73c25a80ca/processing_phi3_v.py:435\u001b[0m, in \u001b[0;36mPhi3VProcessor._convert_images_texts_to_inputs\u001b[0;34m(self, images, texts, padding, truncation, max_length, return_tensors)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m unique_image_ids \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(unique_image_ids)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_ids must start from 1, and must be continuous int, e.g. [1, 2, 3], cannot be \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munique_image_ids\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# total images must be the same as the number of image tags\u001b[39;00m\n\u001b[0;32m--> 435\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(unique_image_ids) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(images), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal images must be the same as the number of image tags, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unique_image_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m image tags and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(images)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m images\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    437\u001b[0m image_ids_pad \u001b[38;5;241m=\u001b[39m [[\u001b[38;5;241m-\u001b[39miid]\u001b[38;5;241m*\u001b[39mnum_img_tokens[iid\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m iid \u001b[38;5;129;01min\u001b[39;00m image_ids]\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minsert_separator\u001b[39m(X, sep_list):\n",
      "\u001b[0;31mAssertionError\u001b[0m: total images must be the same as the number of image tags, got 0 image tags and 1 images"
     ]
    }
   ],
   "source": [
    "temperature = 0.2\n",
    "top_p = 0.7\n",
    "max_new_tokens = 16\n",
    "\n",
    "state, _ = utils_gradio.lvlm_bot(state, temperature, top_p, max_new_tokens)\n",
    "\n",
    "recovered_image = state.recovered_image\n",
    "fig = plt.figure()\n",
    "plt.imshow(recovered_image)\n",
    "role, message = state.messages[-1]\n",
    "print(f'{role}: {message}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560b1616-e445-4695-bdff-1221980d52b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def show_cam_on_image(img: np.ndarray,\n",
    "                      mask: np.ndarray,\n",
    "                      colormap: int = cv2.COLORMAP_JET) -> np.ndarray:\n",
    "    \"\"\" This function overlays the cam mask on the image as an heatmap.\n",
    "    By default the heatmap is in BGR format.\n",
    "    :param img: The base image in RGB or BGR format.\n",
    "    :param mask: The cam mask.\n",
    "    :param use_rgb: Whether to use an RGB or BGR heatmap, this should be set to True if 'img' is in RGB format.\n",
    "    :param colormap: The OpenCV colormap to be used.\n",
    "    :returns: The default image with the cam overlay.\n",
    "    \"\"\"\n",
    "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
    "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
    "    heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "    if np.max(img) > 1:\n",
    "        raise Exception(\n",
    "            \"The input image should np.float32 in the range [0, 1]\")\n",
    "\n",
    "    cam = heatmap + img\n",
    "    cam = cam / np.max(cam)\n",
    "    return np.uint8(255 * cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dba886f-bcd5-4379-a33e-8d6925c7b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_selector = 'all'\n",
    "fn_attention = state.attention_key + '_relevancy.pt'\n",
    "recovered_image = state.recovered_image\n",
    "img_idx = state.image_idx\n",
    "word_rel_maps = torch.load(fn_attention)\n",
    "\n",
    "word_rel_map = word_rel_maps[type_selector]\n",
    "image_list = []\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(4, 4, figsize=(10,10))\n",
    "# fig.tight_layout()\n",
    "\n",
    "for ax, (rel_key, rel_map) in zip(axes.ravel(), word_rel_map.items()):\n",
    "\n",
    "    if (rel_map.shape[-1] != 577):\n",
    "        rel_map = rel_map[-1,:][img_idx:img_idx+576].reshape(1,1,24,24).float()\n",
    "    else:\n",
    "        rel_map = rel_map[0,1:].reshape(1,1,24,24).float()\n",
    "        \n",
    "    rel_map_upscaled = torch.nn.functional.interpolate(rel_map, scale_factor=14, mode='bilinear')\n",
    "    rel_map_upscaled = (rel_map_upscaled - rel_map_upscaled.min()) / (rel_map_upscaled.max() - rel_map_upscaled.min())\n",
    "    rel_map_upscaled = rel_map_upscaled.squeeze(0).squeeze(0)\n",
    "    \n",
    "    original_image = np.array(recovered_image)\n",
    "    original_image = (original_image - original_image.min()) / (original_image.max() - original_image.min())\n",
    "    vis = show_cam_on_image(original_image, rel_map_upscaled.cpu().numpy())\n",
    "    # vis = Image.from_array(vis)\n",
    "    \n",
    "    ax.imshow(vis)\n",
    "    ax.set_title(f'{rel_key}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecefb73f-fd53-4c35-9a00-dec2373f5c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "state.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd010116-b662-4b30-8531-584c5561c8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rel_maps['llama_token'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24933b1f-f025-403d-9420-00efb39bcf85",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_rel_maps.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ed3d7c-eeff-4992-99a0-92dadf2d3dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoProcessor\n",
    "model_id = \"microsoft/Phi-3.5-vision-instruct\" \n",
    "\n",
    "# Note: set _attn_implementation='eager' if you don't have flash_attn installed\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "  model_id, \n",
    "  device_map=\"cuda\", \n",
    "  trust_remote_code=True, \n",
    "  torch_dtype=\"auto\", \n",
    "  _attn_implementation='eager'    \n",
    ")\n",
    "\n",
    "# for best performance, use num_crops=4 for multi-frame, num_crops=16 for single-frame.\n",
    "processor = AutoProcessor.from_pretrained(model_id, \n",
    "  trust_remote_code=True, \n",
    "  num_crops=4\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d72702-4323-4731-81d7-923577191ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.vision_embed_tokens.img_processor.config.output_attentions = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf108c4-a2c8-4171-9ebd-1dc6dd874966",
   "metadata": {},
   "outputs": [],
   "source": [
    "all([hasattr(getattr(model, i) for i in 'model.vision_embed_tokens'.split('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b71c1210-0c39-4ce8-8163-cc86254bf35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['legacy', 'add_prefix_space', '_tokenizer', '_decode_use_source_tokenizer', 'init_inputs', 'init_kwargs', 'name_or_path', '_processor_class', 'model_max_length', 'padding_side', 'truncation_side', 'model_input_names', 'clean_up_tokenization_spaces', 'split_special_tokens', 'deprecation_warnings', '_in_target_context_manager', 'chat_template', '_pad_token_type_id', 'verbose', '_special_tokens_map', 'extra_special_tokens', 'SPECIAL_TOKENS_ATTRIBUTES', '_add_bos_token', '_add_eos_token', 'use_default_system_prompt', 'vocab_file'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.tokenizer.__dict__.keys()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
